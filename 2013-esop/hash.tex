\subsection{Using RCU to ensure linearizability\label{sec:rcu-hash}}

In this section, we apply our logic to a concurrent hash table by Triplett et
al.~\cite{hashtable}. We prove the linearizability of the
algorithm~\cite{linearizability}, which, informally, states that, from the point
of view of its client, the algorithm behaves in the same way as its
straightforward specification by an abstract data type with atomic operations.
The distinguishing characteristic of the hash table algorithm is its ability to
resize the hash table without blocking lookups, achieved using RCU: calls to
$\sync$ during a resize ensure that its data structure manipulations preserve the
linearizability of concurrent lookups. Due to space constraints, we describe the
algorithm informally and only give the main proof idea; a complete pseudocode
and a proof are given in Appendix~\ref{app:hash}.

\subsection{Concurrent resizable hash table}

To concentrate on the core challenges posed by the algorithm, we assume that it
is executed in the presence of a garbage collector, so that the uses of RCU
in it are meant to ensure linearizability. RCU uses for memory reclamation
present in the original algorithm can be handled as in Section~\ref{sec:rcu-inc}.

The hash table uses open-chaining and is represented by a pointer {\tt table} to
a structure containing a pointer to a hash array and its size. Each entry in
the array points to a singly-linked list of nodes representing the corresponding
bucket. The algorithm provides standard methods for looking up a key ({\tt
  lookup}), inserting and removing a key ({\tt insert}, {\tt remove}), and
shrinking and expanding the hash array ({\tt shrink}, {\tt expand}).

The {\tt lookup}, {\tt insert} and {\tt remove} methods follow the standard
algorithms for open-chaining hash tables, computing the desired bucket address
based on the key and traversing the linked list representing the bucket.  The
only addition is the use of RCU: the hash table algorithm executes {\tt lookup}
in a read section, and the rest of the methods in write sections. This allows
lookups to run in parallel with the other operations, but enforces mutual
exclusion among {\tt insert}, {\tt remove} and resizing operations. 
%This restriction can be eased as described in~\cite{hashtable}, but to keep
%presentation tractable, we do not consider this here.
Since lookups run in parallel with hash table modification operations, the
latter always have to preserve the consistency of hash table data structures.

We omit the description of the {\tt shrink} method and proceed to explain {\tt
  expand}, which is the most interesting part of the algorithm. We illustrate
the execution of {\tt expand} using Figure~\ref{fig:cex}, which demonstrates
enlarging a table with a single bucket into one with two buckets.  The method
performs the following sequence of actions:
\begin{figure}[t]
\includegraphics[scale=.32, trim= 0 4.8cm 0 0.15cm]{hash}
\caption{A scenario illustrating the need for RCU to ensure linearizability}
\label{fig:cex}
\end{figure}
\begin{enumerate}
\item Allocate the new, larger table.
\item For each new bucket, search the corresponding old bucket for the first
  entry that hashes to the new bucket, and link the new bucket to that entry
  (Figure~\ref{fig:cex}a). Since all the entries which will end up in the new
  bucket appear in the same old bucket, this constructs an entirely valid new
  hash table, but with multiple buckets zipped together into a single imprecise
  chain.
\item Atomically assign {\tt table} to the new table pointer.  Lookups may now
  traverse the new table, but they will not benefit from any additional
  efficiency until later steps unzip the buckets.
\item Execute $\sync$. All new readers will see the new table, and thus no
  references to the old table will remain.
\item \label{lab} For each bucket in the old table (each of which contains items
  from multiple buckets of the new table):
\begin{enumerate}
\item Advance the old bucket pointer one or more times until it reaches a node
  that doesn't hash to the same bucket as the previous node. Call the previous
  node {\tt prev} (node $1$ in Figure~\ref{fig:cex}a).
\item Find the subsequent node {\tt curr} that does hash to the same bucket as
  node {\tt prev}, or {\tt NULL} if no such node exists (node $3$ in
  Figure~\ref{fig:cex}a).
\item Set {\tt prev}'s next pointer to {\tt curr}, bypassing the nodes which do
  not hash to {\tt prev}'s bucket (Figure~\ref{fig:cex}b).
\end{enumerate}
\item \label{lab2} Execute $\sync$. New readers will see the changes made in
  step~\ref{lab}.
\item If any changes occurred in step~\ref{lab}, repeat from that step.
In our example, this will unzip the buckets as shown in
Figures~\ref{fig:cex}c--\ref{fig:cex}d. 
\end{enumerate}
The key place in this algorithm is the $\sync$ in step~\ref{lab2}. Without it,
lookups traversing the hash table concurrently with {\tt expand} would return
incorrect results. To see this, assume a lookup of the value $3$ starts
traversing the odd bucket in Figure~\ref{fig:cex}a and gets preempted while
looking at node $2$. A concurrent {\tt expand} can then execute step~\ref{lab}
of unzipping, connecting node $1$ to node $3$ (Figure~\ref{fig:cex}b). In the
absence of $\sync$ at step~\ref{lab2}, this could be immediately followed by
another step~\ref{lab}, connecting $2$ to $4$ (Figure~\ref{fig:cex}c). The newly
scheduled lookup would then fail to find node $3$, even though it has been in
the table for the whole duration of the method, which is not a linearizable
result. The trouble is that in this scenario the lookup will see the
modification of the writer changing Figure~\ref{fig:cex}b to~\ref{fig:cex}c, but
not its earlier modification \ref{fig:cex}a$\,\rightarrow\,$\ref{fig:cex}b that
made node $3$ reachable directly from node $1$.  The $\sync$ after the
modification \ref{fig:cex}a$\,\rightarrow\,$\ref{fig:cex}b ensures that lookups
searching for node $3$ will have to use the newly established link to this node
and will not miss it.  Stated more generally, the $\sync$ in step~\ref{lab2} of
the algorithm ensures that readers see the changes in the order the writer makes
them.

\subsection{Proof idea}

\ag{Rewrite}

We again prove the algorithm in the logic instantiated with the algebra
$\RAM_{\pi}$ of Section~\ref{sec:perm}. In our proof, a reader is viewed as
taking read-only permissions from the shared state preventing certain memory
cells from being modified during a read section.

We explain the key idea of the proof using the example in Figure~\ref{fig:cex}.
Consider a {\tt lookup} operation traversing a bucket in a read section, e.g.,
the odd bucket in Figure~\ref{fig:cex}a, looking for a particular odd node.  A
concurrent {\tt expand} method can link odd elements in the bucket to odd ones,
or even to even. As the example discussed earlier illustrates, the latter kind
of modifications (such as linking $2$ to $4$) needs to be prohibited for the
duration of the read section. To this end, when the RCU reader gets a reference
to the bucket it is interested in, it takes out the read-only permissions for
the next node pointers of all nodes in the bucket that have a wrong hash (e.g.,
$2$ and $4$ from the odd bucket in Figure~\ref{fig:cex}a). This ensures that
{\tt expand} cannot splice out any elements with the hash the reader is
interested in (e.g., $3$ in
\ref{fig:cex}b$\,\rightarrow\,$\ref{fig:cex}c). Since {\tt insert} and {\tt
  delete} cannot run concurrently with {\tt expand}, the set of nodes in the
bucket with this hash will stay the same throughout the part of the lookup
traversal overlapping with {\tt expand}. This allows us to prove that the lookup
returns a correct value.

As usual, the read-only permissions taken by a reader are placed in its
custodian local state, ensuring that it will return them to the shared state at
the end of its critical section. The shadow shared state maintains a copy of all
permissions taken; thus, the count in the master permission for a node in the
real shared state is equal to the number of read-only permissions for it in the
shadow shared state. This invariant allows us to reason about the $\sync$
operations in {\tt expand} as follows.

The proof of the {\tt expand} method, which runs in an RCU write section, takes
the actions of readers into account using the same mechanism of rely-guarantee
reasoning we used in Section~\ref{sec:list}. In step~\ref{lab} of the algorithm,
the writer always changes the next field of a node that is only reachable from
the bucket corresponding to its hash code, e.g., node $1$ in
\ref{fig:cex}a$\,\rightarrow\,$\ref{fig:cex}b. This change is possible, as a reader can
only take permissions for next fields of nodes reachable from a bucket with a
hash different from theirs. However, the writer cannot directly follow up this
modification with another one on the same bucket, such as changing the next
field of node $2$ in \ref{fig:cex}b$\,\rightarrow\,$\ref{fig:cex}c. Even though
this node is now only reachable from the even bucket (Figure~\ref{fig:cex}b), it
used to be reachable from both buckets in a previous state (Figure~\ref{fig:cex}a),
and, hence, readers might have taken read-only permissions for it. The $\sync$
in step~\ref{lab2} forces all such readers to return their permission to the
real shared state. According to the invariant relating the real and the shadow
states, this upgrades the permission for the nodes to be modified in the next
step~\ref{lab}, such as node $2$ in Figure~\ref{fig:cex}b, to full ones.  Since
these nodes are now only reachable from the right buckets (the even one in the
case of $2$), the new readers cannot take read-only permissions for the
nodes again and the writer can proceed with the modification.


%%% Local Variables:
%%% TeX-master: "recycling"
%%% End:  
