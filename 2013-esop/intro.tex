%!TEX root = ./recycling.tex
%
% intro.tex

\begin{abstract}
  Memory management is one of the most complex aspects of modern concurrent
  algorithms, and various techniques proposed for it---such as hazard pointers,
  read-copy-update and epoch-based reclamation---have proved very challenging
  for formal reasoning. In this paper, we show that different memory reclamation
  techniques actually rely on the same implicit synchronisation pattern, not
  clearly reflected in the code, but only in the form of assertions used to
  argue its correctness. The pattern is based on the key concept of a {\em grace
    period}, during which a thread can access certain shared memory cells
  without fear that they get deallocated. We propose a modular reasoning method,
  motivated by the pattern, that handles all three of the above memory
  reclamation techniques in a uniform way. By explicating their fundamental
  core, our method achieves clean and simple proofs, scaling even to
  realistic implementations of the algorithms without a significant increase in
  proof complexity. We formalise the method using a combination of separation
  logic and temporal logic and use it to verify example instantiations of the
  three approaches to memory reclamation.

  % We propose a method for verifying concurrent memory reclamation algorithms
  % that yields simple and clean proofs. Our key insight is the identification of
  % an implicit synchronisation pattern, common to different memory management
  % implementations, which guarantees to a thread that, during a certain {\em
  %   grace period}, the memory cell it is accessing will not be deallocated. We
  % express the contract among threads enforced by this pattern as a temporal
  % assertion of a restricted form, and use it as a basis for a modular reasoning
  % principle. We formalise our method in a combination of separation logic and
  % temporal logic and use it to verify example instantiations of three approaches
  % to memory---hazard pointers, read-copy-update and epoch-based reclamation.
  % Our proofs elucidate the key insight behind these memory management
  % techniques.
\end{abstract}

\section{Introduction\label{sec:intro}}

Non-blocking synchronisation is a style of concurrent programming that
avoids the blocking inherent to lock-based mutual exclusion. Instead, it uses
low-level synchronisation techniques, such as compare-and-swap operations, that
lead to more complex algorithms, but provide a better performance in the
presence of high contention among threads. Non-blocking synchronisation is
primarily employed by concurrent implementations of data structures, such as
stacks, queues, linked lists and hash tables.

%(e.g., in the \textsf{java.util.concurrent} library for Java and Threading
%Building Blocks for C++).

Reasoning about concurrent programs is generally difficult, because of the need
to consider all possible interactions between concurrently executing threads.
This is especially true for non-blocking algorithms, where threads interact in
subtle ways through dynamically-allocated data structures. In the last few
years, great progress has been made in addressing this challenge. We now have
a number of logics %~\cite{rgsep,sagl,lrg,dg,cap,hazard-feng} 
and automatic tools %~\cite{roman,smallfootrg,hyperv,turkish} 
that combat the complexity of
non-blocking algorithms by verifying them {\em thread-modularly}, i.e., by
considering every thread in an algorithm in isolation under some assumptions on
its environment and thus avoiding explicit reasoning about all thread
interactions. Not only have such efforts increased our confidence in the
correctness of the algorithms, but they have often resulted in
human-understandable proofs that elucidated the core design principles behind
these algorithms.

However, one area of non-blocking concurrency has so far resisted attempts to
give proofs with such characteristics---that of {\em memory
  management}. By their very nature, non-blocking algorithms allow access to
memory cells while they are being updated by concurrent threads.  Such
optimistic access makes memory management one of the most complex aspects of the
algorithms, as it becomes very difficult to decide when it is safe
to reclaim a memory cell. Incorrect decisions can lead to errors such as memory
access violations, corruption of shared data and return of incorrect results.
To avoid this, an algorithm needs to include a protocol for coordinating between
threads accessing the shared data structure and those trying to reclaim its
nodes. Relying on garbage collection is not always an option, since non-blocking
algorithms are often implemented in languages without it, such as C/C++.

In recent years, several different methods for explicit memory reclamation in
non-blocking algorithms have been proposed:
\begin{itemize}
\vspace{-2pt}
\item {\bf\em Hazard pointers}~\cite{hazard} let a thread publish the address of
  a node it is accessing as a special global pointer. Another thread wishing to
  reclaim the node first checks the hazard pointers of all threads.
\item {\bf\em Read-copy-update (RCU)}~\cite{rcu-thesis} lets a thread mark a
  series of operations it is performing on a data structure as an RCU critical
  section, and provides a command that waits for all threads currently in 
  critical sections to exit them. A thread typically accesses a given node
  inside the same critical section, and a reclaimer waits for all threads to
  finish their critical sections before deallocating the node.
\item {\bf\em Epoch-based reclamation}~\cite{fraser-thesis} uses a special
  counter of epochs, approximating the global time, for quantifying how long ago
  a given node has been removed from the data structure. A node that has been
  out of the data structure for a sufficiently long time can be
  safely deallocated.
\end{itemize}
\vspace{-2pt}

Despite the conceptual simplicity of the above methods, their implementations in
non-blocking algorithms are extremely subtle. For example, as we explain in
\S\ref{sec:informal}, the protocol for setting a hazard pointer is more involved
than just assigning the address of the node being accessed to a global variable.
Reasoning naturally about protocols so subtle is very challenging. Out of the
above algorithms, only restricted implementations of hazard pointers have been
verified~\cite{parkinson-hazard,hazard-feng,turkish,kiv}, and even in this case,
the resulting proofs were very complicated (see \S\ref{sec:relwork}
for discussion).

The memory reclamation algorithms achieve the same goal by intuitively
similar means, yet are very different in details. In this paper, we show that,
despite these differences, the algorithms actually rely on the same
synchronisation pattern that is {\em implicit}---not clearly reflected in the
code, but only in the form of assertions used to argue its correctness. We
propose a modular reasoning method, formalising this pattern, that handles all
three of the above approaches to memory reclamation in a uniform way. By
explicating their fundamental core, we achieve % our method achieves 
clean and
simple proofs, scaling even to realistic implementations of the algorithms
without a significant increase in proof complexity.

In more detail, we reason about memory reclamation algorithms by formalising the
concept of a {\em grace period}---the period of time during which a given thread
can access certain nodes of a data structure without fear that they get
deallocated.
%\footnote{The term ``grace'' comes from RCU~\cite{rcu-thesis}, although here we
%  give it a slightly different meaning; see \S\ref{sec:rcu-prelim}
%  \longonly{~for discussion}.}  
Before deallocating a node, a reclaimer needs to wait until the grace periods of
all threads that could have had access to the node pass.  Different approaches
to memory reclamation define the grace period in a different way. However, we
show that, for the three approaches above, the duration of a grace period can be
characterised by a temporal formula of a fixed form ``$\eta \since \mu$'', e.g.,
``the hazard pointer has pointed to the node since the node was present in the
shared data structure''. This allows us to express the contract between threads
accessing nodes and those trying to reclaim them by an invariant stating that a
node cannot be deallocated during the corresponding grace period for any
thread. The invariant enables modular reasoning: to prove the whole algorithm
correct, we just need to check that separate threads respect it. Thus, a thread
accessing the data structure has to establish the assertion ``$\eta \since
\mu$'', ensuring that it is inside a grace period; a thread wishing to reclaim a
node has to establish the {\em negation} of such assertions for all threads,
thus showing that all grace periods for the node have passed. Different
%reclamation 
algorithms just
implement code that establishes assertions of the same form in different ways.

We formalise such correctness arguments in a modular program logic, combining
one of the concurrent versions of separation logic~\cite{rgsep,sagl} with
temporal logic (\S\ref{sec:logic}). We demonstrate our reasoning method by
verifying example instantiations of the three approaches to memory
reclamation---hazard pointers (\S\ref{sec:grace}), RCU (\S\ref{sec:rcu}) and
epoch-based reclamation \tra{\ref{app:epoch}}{\nepoch}. %(\S\ref{sec:epoch}).
In particular, for RCU we provide the first specification of its interface that
can be effectively used to verify common RCU-based algorithms. Due to space
constraints, the development for epochs is deferred to
\tr{\ref{app:epoch}}{\nepoch}.
%; there we also give an informal summary of the proof. 
As far as we know, the only other algorithm that allows explicitly returning
memory to the OS in non-blocking algorithms is the Repeat-Offender
algorithm~\cite{rop}. Our preliminary investigations show that our method is
applicable to it as well; we leave formalisation for future work.

% We note that the need for a mixture of spatial and temporal reasoning for
% verifying reclamation algorithms has already been realised by previous attempts
% to tackle hazard pointers~\cite{hazard-feng,kiv}, whose influence we fully
% acknowledge (we discuss related work in detail in \S\ref{sec:relwork}).
% However, this paper is the first to formalise a pattern common to different
% algorithms and thereby achieve a simple and scalable reasoning about
% them. Unlike prior proposals, our formalisation enriches the existing spatial
% reasoning principles only with the minimum amount of temporal reasoning
% (essentially, involving only the {\sf since} connective) necessary to express
% the pattern. The right mixture allows us to establish and maintain the required
% temporal assertions in our proofs with minimum effort, using mostly non-temporal
% reasoning.

\setlength{\abovedisplayskip}{7pt plus 2pt minus 2pt}
\setlength{\belowdisplayskip}{7pt plus 2pt minus 2pt}
\setlength{\abovedisplayshortskip}{0pt plus 2pt}
\setlength{\belowdisplayshortskip}{0pt plus 2pt}

% From a more general perspective, our results provide an instructive illustration
% of the interplay between {\em programming patterns} and {\em proof patterns}: we
% use an intuitive understanding of the commonalities among different algorithms
% to motivate the design of a verification method; in its turn, the patterns of
% assertions occurring in the resulting proofs let us elucidate the core of the
% algorithms and pinpoint the commonality precisely.  We hope that our approach
% will be similarly useful in simplifying the verification of other challenging
% concurrent algorithms.



% What is difficult about verifying non-blocking memory reclamation algorithms is
% that the complicated protocols used in them call for temporal reasoning, whereas
% most modular logics provide only variants of rely-guarantee, where a thread's
% transitions are specified by relations. This need for a temporal approach has
% already been realised by previous attempts to verify hazard
% pointers~\cite{hazard-feng,kiv}. The challenge is that, for proofs to be natural
% and simple, the temporal assertions used in them have to reflect the algorithm
% structure and the intention of its designer: make a wrong choice, and the
% complexity of the proof gets quickly out of hand, especially for realistic
% implementations.


%%% Local Variables:
%%% TeX-master: "recycling"
%%% mode: latex
%%% End:  
